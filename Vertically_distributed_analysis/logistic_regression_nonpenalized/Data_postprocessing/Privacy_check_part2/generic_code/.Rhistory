set.seed(1)
n <- 1000
beta <- c(2, 1, -3, 1.5,3,-2,-1.8)
p <- length(beta) - 1
X <- matrix(rnorm(n * p, mean=0, sd=1), ncol=p)
x_1 <- X[,1:3]
x_1 <- cbind(rep(1,n),x_1)
x_2 <- X[,4:6]
View(x_1)
set.seed(1)
# parametres initiaux
n <- 1000
beta <- c(2, 1, -3, 1.5,3,-2,-1.8)
p <- length(beta) - 1
# data
X <- matrix(rnorm(n * p, mean=0, sd=1), ncol=p)
x_1 <- X[,1:3]
# colonne de 1 (intercept) + 3 predicteurs
x_1 <- cbind(rep(1,n),x_1)
# 3 predicteur
x_2 <- X[,4:6]
# vecteur reponse
y <- 2 * rbinom(prob=1/(1+exp(- cbind(1,X) %*% beta)), n=n, size=1) - 1
#Déterminer la valeur du paramètre de régularisation
#(pour être cohérent avec l'article, lamda=0.0001 sera utilisé pour comparer avec une
#régression sans pénalisation)
lamda <- 0.000001
################# OPÉRATIONS AUX SITES : LOCAL GRAM MATRIX K_k envoyée au noeud central
K_1 <- x_1%*%t(x_1)
K_2 <- x_2%*%t(x_2)
K_all <- K_1+K_2
#Quantité en mémoire qui sera réutilisée dans le gradient et la hessienne
#(pour éviter de recalculer à chaque fois)
short <- (1/lamda)*(diag(y))%*%K_all%*%(diag(y))
alpha_u <- rep(0.5,1000)
converge <- FALSE
while (!converge) {
success <- FALSE
pas <- 2
while (!success) {
pas <- pas/2
alpha_u1 <- as.vector(alpha_u - pas*(solve(short+diag(1/(alpha_u*(1-alpha_u))),(short%*%alpha_u+(log(alpha_u/(1-alpha_u)))))))
success <- all(alpha_u1 > 0 & alpha_u1<1)
}
#converge <- all(abs(alpha_u1-alpha_u)<(10^(-7)))
converge <- norm((short%*%alpha_u1+(log(alpha_u1/(1-alpha_u1)))),type="2")<(10^(-6))
alpha_u <- alpha_u1
}
beta_1_u <- (1/lamda)*t(x_1)%*%diag(alpha_u)%*%y
c_site2 <- (1/lamda)*K_2%*%diag(alpha_u)%*%y
###On peut vérifier que le rang est bien égal au nombre de paramètres du site
rankMatrix(x_2)
??rankMatrix
library(Matrix)
###On peut vérifier que le rang est bien égal au nombre de paramètres du site
rankMatrix(x_2)
###On va identifier un système linéairement indépendant pour résoudre les BETA(K)
q <- qr(t(x_2))
tx2 <- t(x_2)
x2_indep <- tx2[,q$pivot[seq(q$rank)]]
beta_site2_local <- solve(t(x2_indep),c_site2[q$pivot[seq(q$rank)]])
################# COMPARER AVEC GLM (SI LE PARAMÈTRE DE RÉGULARIZATION A ÉTÉ FIXÉ BAS)
beta_1_u
beta_site2_local
summary(glm((y+1)/2 ~ X, family="binomial"))$coefficients[,1]
X
alpha_fin <- alpha_u
V <- (1/lamda)*K_all%*%diag(alpha_fin)%*%y
V_centre <- as.vector((exp(V)/(1+exp(V)))*(1-(exp(V)/(1+exp(V)))))
S_inv <- solve(solve(diag(V_centre))+(1/(2*lamda))*K_all)
err_x_1 <- as.vector(diag(t(x_1)%*%S_inv%*%x_1))
err_x_2 <- as.vector(diag(t(x_2)%*%S_inv%*%x_2))
err_x_all <- c(err_x_1,err_x_2)
err_beta <- sqrt(rep((1/(2*lamda)),length(err_x_all))-(1/(4*lamda^2))*err_x_all)
err_beta
summary(glm((y+1)/2 ~ X, family="binomial"))$coefficients[,2]
#Pour les tests, data_k est la matrice de données centralisée ici, où la première colonne est la
#variable de réponse y codée en version 0 et 1
burn1000
library(aplore3)
data_k <- burn1000
library(aplore3)
library(glmnet)
#Pour les tests, data_k est la matrice de données centralisée ici, où la première colonne est la
#variable de réponse y codée en version 0 et 1
burn1000
data_k <- burn1000
data_k <- data_k[,2:9]
data_k$death <- as.numeric(data_k$death)
data_k$death[which(data_k$death==2)] <- 0
data_k$gender <- as.numeric(data_k$gender)
data_k$gender[which(data_k$gender==2)] <- 0
data_k$race <- as.numeric(data_k$race)
data_k$race[which(data_k$race==2)] <- 0
data_k$inh_inj <- as.numeric(data_k$inh_inj)
data_k$inh_inj[which(data_k$inh_inj==2)] <- 0
data_k$flame <- as.numeric(data_k$flame)
data_k$flame[which(data_k$flame==2)] <- 0
#Extraire y et coder en version -1 et 1 pour VERTIGO
y <- data_k[,2]
y[which(y==0)] <- -1
y <- as.numeric(as.vector(y))
#Centrer-réduire les covariables comme on fait une régression Ridge
data_k[,c(1,3:8)] <- scale(data_k[,c(1,3:8)])
#Séparer la matrice en 2 sites pour les exemples - Augmenter la matrice de
#données du second site d'une colonne de 1 pour l'intercept
x_1 <- as.matrix(data_k[,c(1,3,4)])
x_2 <- as.matrix(cbind(data_k[,5:8],rep(1,1000)))
#Séparer la matrice en 2 sites pour les exemples - Augmenter la matrice de
#données du second site d'une colonne de 1 pour l'intercept
x_1 <- as.matrix(data_k[,c(1,3,4)])
head(x_1)
x_2 <- as.matrix(cbind(data_k[,5:8],rep(1,1000)))
head(x_2)
#Déterminer la valeur du paramètre de régularisation
#(pour être cohérent avec l'article, lamda=0.0001 sera utilisé pour comparer avec une
#régression sans pénalisation)
lamda <- 0.0001
################# OPÉRATIONS AUX SITES : LOCAL GRAM MATRIX K_k envoyée au noeud central
K_1 <- x_1%*%t(x_1)
K_2 <- x_2%*%t(x_2)
################# OPÉRATIONS AUX SITES : LOCAL GRAM MATRIX K_k envoyée au noeud central
K_1 <- x_1%*%t(x_1)
K_2 <- x_2%*%t(x_2)
K_all <- K_1+K_2
#Quantité en mémoire qui sera réutilisée dans le gradient et la hessienne
#(pour éviter de recalculer à chaque fois)
short <- (1/lamda)*(diag(y))%*%K_all%*%(diag(y))
alpha_u <- rep(0.5,1000)
nbiter <- 100
for (i in 1:nbiter) {
alpha_u <- as.vector(alpha_u - (solve(short+diag(1/(alpha_u*(1-alpha_u))),(short%*%alpha_u+(log(alpha_u/(1-alpha_u)))))))
alpha_u[which(alpha_u<0)] <- 0.000000001
alpha_u[which(alpha_u>1)] <- 0.999999999
#alpha_u <- 1/(1+exp(-alpha_u))
}
alpha_u
alpha_u
beta_1_u <- (1/lamda)*t(x_1)%*%diag(alpha_u)%*%y
beta_2_u <- (1/lamda)*t(x_2)%*%diag(alpha_u)%*%y
################# COMPARER AVEC GLM (SI LE PARAMÈTRE DE RÉGULARIZATION A ÉTÉ FIXÉ BAS)
s1 <- glm(death~.,data_k,family = "binomial")
coef(s1)
summary(s1)
s2 <- glmnet(data_k[,c(1,3:8)], data_k$death, family = "binomial", alpha = 0, lambda = 0.0001)
coef(s2)
summary(s2)
beta_1_u
beta_2_u
all_beta_u <- rbind(beta_1_u, beta_2_u)
all_beta_u
coef(s1)
data_k
data_k[,c(1,3:8)]
coef(s2)
all_beta_u
summary(s1)
####Reverse engineering exemple : Tentons de retracer les données, en assumant que VERTIGO a été roulé
alpha_fin <- alpha_u
V <- (1/lamda)*K_all%*%diag(alpha_fin)%*%y
V_centre <- as.vector((exp(V)/(1+exp(V)))*(1-(exp(V)/(1+exp(V)))))
#############Les noeuds ont aussi calculé le vecteur diagonal V_sites en utilisant les coefficients beta
V_noeud1 <- exp(x_1%*%beta_1_u)
V_noeud2 <- exp(x_2%*%beta_2_u)
V_tot <- V_noeud1*V_noeud2
V_sites <- (V_tot/(1+V_tot))*(1-(V_tot/(1+V_tot)))
as.vector(V_sites^(1/2))
A_1 <- t(x_1)%*%(diag(as.vector(V_sites^(1/2))))
A_2 <- t(x_2)%*%(diag(as.vector(V_sites^(1/2))))
#On retrace les données
x_1_server <- t(as.matrix(A_1)%*%diag(V_centre^(-1/2)))
x_2_server <- t(as.matrix(A_2)%*%diag(V_centre^(-1/2)))
#On vérifie la différence avec la matrice de données initiale
length(which(x_1_server-x_1 > 0.0001))
length(which(x_2_server-x_2 > 0.0001))
#On peut aussi vérifier la différence entre les matrices V
length(which(V_sites-V_centre > 0.0001))
V_sites
V_centre
library(CVXR)
library(tidyverse)
library(glmnet)
library(matrixcalc)
library(aplore3)
library(CVXR)
library(tidyverse)
library(glmnet)
library(matrixcalc)
library(aplore3)
data_k <- burn1000
data_k <- data_k[,2:9]
data_k$death <- as.numeric(data_k$death)
data_k$death[which(data_k$death==2)] <- 0
data_k$gender <- as.numeric(data_k$gender)
data_k$gender[which(data_k$gender==2)] <- 0
data_k$race <- as.numeric(data_k$race)
data_k$race[which(data_k$race==2)] <- 0
data_k$inh_inj <- as.numeric(data_k$inh_inj)
data_k$inh_inj[which(data_k$inh_inj==2)] <- 0
data_k$flame <- as.numeric(data_k$flame)
data_k$flame[which(data_k$flame==2)] <- 0
X <- as.matrix(data_k)
#Extraire y et coder en version -1 et 1 pour VERTIGO
y <- data_k[,2]
y[which(y==0)] <- -1
y <- as.numeric(as.vector(y))
#Centrer-réduire les covariables comme on fait une régression Ridge
X[,c(1,3:8)] <- scale(X[,c(1,3:8)])
X <- X[,-2]
#set.seed(1)
n <- 1000
#beta <- c(2, 1, -3, 1.5,3,-2,-1.8)
#p <- length(beta) - 1
#X <- matrix(rnorm(n * p, mean=0, sd=1), ncol=p)
x1 <- X[,1:2]
x2 <- X[,3:7]
lambda <- 0.02
glmnet_model <- glmnet(X, (y+1)/2, family=binomial, lambda=lambda, alpha=0, standardize=FALSE)
# Sites-------------------------------------------------------------------------
k1 <- x1 %*% t(x1)
k2 <- x2 %*% t(x2)
alpha <- Variable(n)
K <- k1+k2
Q <- 1/(2*lambda*n) * diag(y) %*% K %*% diag(y)
objective <- Minimize(
quad_form(alpha, Q)
- sum(entr(1-alpha) + entr(alpha))
# - sum(-(1-alpha)*log(1-alpha) - alpha*log(alpha))
)
constraint1 <- alpha >= 0
constraint2 <- alpha <= 1
constraint3 <- sum(y * alpha) == 0
problem <- Problem(objective, constraints=list(constraint1, constraint2, constraint3))
solution <- solve(problem)
print(solution$status)
alpha_hat <- solution$getValue(alpha)[,1]
alpha_glmnet <- predict(glmnet_model, X, type="response")
# Site 1 (serveur)--------------------------------------------------------------
beta_0_hat <- 1/y[1] * (log(1/alpha_hat[1] - 1) - y[1] * 1/(lambda*n) * (K %*% diag(alpha_hat) %*% y)[1])
beta_hat_1 <- 1/(lambda*n) * t(x1) %*% diag(alpha_hat) %*% y
beta_hat_2 <- 1/(lambda*n) * t(x2) %*% diag(alpha_hat) %*% y
print(coef(glmnet_model)[,1])
print(c(beta_0_hat, beta_hat_1,beta_hat_2))
setwd("C:/Users/morj2519/Downloads/statistique-main/statistique-main/Vertical_Logistic_Regression/Modifications_privacy_for_Git_05062025")
############### Distributed inference ####################
############### Covariate-node code - Privacy assessment - Part 1 ###########################
## License: https://creativecommons.org/licenses/by-nc-sa/4.0/
## Copyright: GRIIS / Université de Sherbrooke
# Loading packages and setting up core variables --------------------------
# Currently, the automated node number allocation currently requires execution in R studio and rstudioapi package
# https://cran.r-project.org/package=rstudioapi
# If you want to skip the automated working directory setting, input 1 here.
# If you do so, make sure the working directory is set correctly manualy.
manualwd <- -1
# If you want to override the node numbering based on filename, input 0 or a positive integer here
manualk <- -1
# No modifications should be required below this point
###########################
if (manualwd != 1) {
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically set working directory
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
} else {
print("The automated working directory setup has been bypassed. If there is an error, this might be the cause.")
}
# Once the working directory as been set, save it so we can pass it to other files
path <- paste0(getwd(), "/")
k <- -1
# If there is a manual override, the node number (k) is set to the manual value --------------------------
if (manualk >= 0) {
k <- manualk
# If there is no valid override number, there will be an attempt to extract the node number from the data file name
} else {
# List all the data files conforming the the pattern below. There should be only 1
datafileslist <- list.files(path=path, pattern="Data_node_[[:digit:]]+.csv")
# Assuming there is only one data file found
if (length(datafileslist) == 1) {
filename <- datafileslist[[1]]
lastunders <- max(unlist(gregexpr("_",filename)))
lenmainfilename <- nchar(filename)-4
autok <- strtoi(substring(filename,lastunders+1,lenmainfilename))
k <- autok
# If there is more than one data file in the folder, the script will halt.
} else {
stop("There is more than one data file in this folder, the node number cannot be automatically identified")
}
}
# Verifying that a valid node number and sequence numbers could be allocated manually or automatically
if (k >= 0) {
source("Data_node_privacy_part1_core_log-regV.R")
data_privacy_log_reg(manualwd,k,path)
} else {
stop("Node numbering was not set properly")
}
setwd("~/GitHub/Elo_Propre/Distrib_analysis/Vertically_distributed_analysis/logistic_regression/Data_postprocessing/Privacy_check_part2/generic_code")
