n <- nrow(node_data)
# Verifying if weights are available. If not, use values of 1s as uniform weights.
if (file.exists(paste0("Weights_node_", k, ".csv"))) {
node_weights <- read.csv(paste0("Weights_node_", k, ".csv"))[,1]
} else {
node_weights <- rep(1, n)
}
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/random_data_same_folder/distributed")
# QuickRun
maxiter = 10
for(i in 1:maxiter){
source("Data_node_call_log-reg_1.R")
source("Data_node_call_log-reg_2.R")
source("Data_node_call_log-reg_3.R")
source("Coord_node_call_iter_log-reg.R")
}
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/examples/random_data_with_weights_same_folder/distributed")
# QuickRun
maxiter = 10
for(i in 1:maxiter){
source("Data_node_call_log-reg_1.R")
source("Data_node_call_log-reg_2.R")
source("Data_node_call_log-reg_3.R")
source("Coord_node_call_iter_log-reg.R")
}
sigmoid <- function(x) {
exp(x) / (1 + exp(x))
}
logreg_D <- function(beta, X, y, W) {
n <- nrow(X)
t(X) %*% W %*% (y - sigmoid(X %*% beta)) # / n
}
logreg_V <- function(beta, X, W) {
n <- nrow(X)
sig <- sigmoid(X %*% beta)[,1]
t(X) %*% W %*% diag(sig*(1-sig)) %*% X # / n
}
if (manualwd != 1) {
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically allocate node number
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
} else {
print("The automated working directory setup has been bypassed. If there is an error, this might be the cause.")
}
node_data <- read.csv(paste0("Data_node_", k, ".csv"))
k=1
t=1
node_data <- read.csv(paste0("Data_node_", k, ".csv"))
n <- nrow(node_data)
# Verifying if weights are available. If not, use values of 1s as uniform weights.
if (file.exists(paste0("Weights_node_", k, ".csv"))) {
node_weights <- read.csv(paste0("Weights_node_", k, ".csv"))[,1]
} else {
node_weights <- rep(1, n)
}
beta_t <- read.csv(paste0("Coord_node_iter_", t, "_primer.csv"))[,1]
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/random_data_same_folder/data_MatchIt")
DataComplete = lalonde
data("lalonde", package = "MatchIt")
DataComplete = lalonde
table(treat)
table(DataComplete$treat)
table(DataComplete$married)
DataComplete$treat
sample(1:nrow(DataComplete))
set.seed(34)
DataComplete = DataComplete[sample(1:nrow(DataComplete))]
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
set.seed(34)
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
set.seed(34)
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
set.seed(34)
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
setseed(34)
set.seed(34)
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
set.seed(34)
DataComplete = lalonde
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
set.seed(34)
DataComplete = lalonde
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
set.seed(34)
DataComplete = lalonde
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
data("lalonde", package = "MatchIt")
set.seed(34)
DataComplete = lalonde
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
# Groupes
Data1 = DataComplete[1:200,]
Data1W <- Data1 %>% select(treat, nodegree, re78)
Data1LR <- Data1 %>% select(married, treat)
table(Data1W$treat)
table(Data1LR$married)
data("lalonde", package = "MatchIt")
set.seed(34)
DataComplete = lalonde
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
# Groupes
Data1 = DataComplete[1:200,]
Data1W <- Data1 %>% select(treat, nodegree, re78)
Data1LR <- Data1 %>% select(married, treat)
table(Data1W$treat)
table(Data1LR$married)
write.csv(Data1W, file = "Data_node_1.csv", row.names = F)
write.csv(Data1LR, file = "Data_node_1_RL.csv", row.names = F)
Data2 = DataComplete[201:400,]
Data2W <- Data2 %>% select(married, nodegree, re78)
Data2LR <- Data2 %>% select(married, treat)
table(Data2W$treat)
table(Data2LR$married)
write.csv(Data2W, file = "Data_node_2.csv", row.names = F)
write.csv(Data2LR, file = "Data_node_2_RL.csv", row.names = F)
table(Data2W$treat)
table(Data1W$treat)
data("lalonde", package = "MatchIt")
set.seed(34)
DataComplete = lalonde
DataComplete = DataComplete[sample(1:nrow(DataComplete)),]
# Groupes
Data1 = DataComplete[1:200,]
Data1W <- Data1 %>% select(treat, nodegree, re78)
Data1LR <- Data1 %>% select(married, treat)
table(Data1W$treat)
table(Data1LR$married)
write.csv(Data1W, file = "Data_node_1.csv", row.names = F)
write.csv(Data1LR, file = "Data_node_1_RL.csv", row.names = F)
Data2 = DataComplete[201:400,]
Data2W <- Data2 %>% select(treat, nodegree, re78)
Data2LR <- Data2 %>% select(married, treat)
table(Data2W$treat)
table(Data2LR$married)
write.csv(Data2W, file = "Data_node_2.csv", row.names = F)
write.csv(Data2LR, file = "Data_node_2_RL.csv", row.names = F)
Data3 = DataComplete[-(1:400),]
Data3W <- Data3 %>% select(treat, nodegree, re78)
Data3LR <- Data3 %>% select(married, treat)
table(Data3W$treat)
table(Data3LR$married)
write.csv(Data3W, file = "Data_node_3.csv", row.names = F)
write.csv(Data3LR, file = "Data_node_3_RL.csv", row.names = F)
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/random_data_same_folder/distributed")
# QuickRun
maxiter = 10
for(i in 1:maxiter){
source("Data_node_call_log-reg_1.R")
source("Data_node_call_log-reg_2.R")
source("Data_node_call_log-reg_3.R")
source("Coord_node_call_iter_log-reg.R")
}
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/random_data_same_folder/pooled")
############### Distributed inference ####################
############### Demo script ##############################
## License: https://creativecommons.org/licenses/by-nc-sa/4.0/
## Copyright: GRIIS / Universit√© de Sherbrooke
# This assumes three data node files in the same folder named as below
# The output is visible in RStudio console
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically set working directory
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
# Pooling data for comparison with pooled model
data_pooled <- rbind(read.csv(paste0("Data_node_1.csv")),
read.csv(paste0("Data_node_2.csv")),
read.csv(paste0("Data_node_3.csv")))
# Verifying if weights are available. If not, use values of 1s as uniform weights.
if (file.exists(paste0("Weights_pooled.csv"))) {
weights_pooled <- read.csv("Weights_pooled.csv")[,1]
} else {
weights_pooled <- rep(1, nrow(data_pooled))
}
# Fitting and printing pooled model
print("Pooled logistic regression results:")
fit <- glm(out1 ~ ., data=data_pooled, family="binomial", weights = weights_pooled)
print(summary(fit)$coefficients)
print("Confidence intervals")
print(confint.default(fit))
predict(fit, data_pooled, type="response")
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/examples/random_data_with_weights_same_folder/distributed")
# QuickRun
maxiter = 10
for(i in 1:maxiter){
source("Data_node_call_log-reg_1.R")
source("Data_node_call_log-reg_2.R")
source("Data_node_call_log-reg_3.R")
source("Coord_node_call_iter_log-reg.R")
}
k=1
t=1
# Expecting data file name like Data_node_1.csv where 1 is the variable k above
# Construct file name according to node data
# Assumes default parameters, like header and separator
node_data <- read.csv(paste0("Data_node_", k, ".csv"))
n <- nrow(node_data)
# Lists all the weight files provided by the user. There should be either none or 1.
Userwlist <- list.files(pattern=paste0("Weights_node_", k, ".csv"))
Userwlist
nbUserwfiles <- length(Userwlist)
# Assumes there is at most one weight file provided by the user found
if (nbUserwfiles > 1){
stop("There is more than one IPW file in this folder, the weights cannot be automatically identified")
}
# Lists all the IPW files conforming the the pattern below. There should be either none or 1.
IPWfilelist <- list.files(pattern=paste0("IPW_node_", k, "_iter_[[:digit:]]+.csv"))
nbIPWfiles <- length(IPWfilelist)
# Assumes there is at most one IPW file found
if (nbIPWfiles > 1) {
stop("There is more than one IPW file in this folder, the weights cannot be automatically identified")
}
# Number of files related to weights
nbWeightfiles <- nbUserwfiles + nbIPWfiles
nbWeightfiles
# Assumes there is at most one type of weight file found
if (nbWeightfiles > 1){
stop("There is nore than one type of weight files in this folder, the weights cannot be automatically identified.")
}
# Find which weights should be used, if any.
# First case checked is for weights provided by the user
if (file.exists(paste0("Weights_node_", k, ".csv"))) {
node_weights <- read.csv(paste0("Weights_node_", k, ".csv"))[,1]
# Second case is for IPW/ITPW
} else if(length(IPWfilelist)>0) {
filename <- IPWfilelist[[1]]
lastunders <- max(unlist(gregexpr("_",filename)))
lastdot <- max(unlist(gregexpr(".", filename, fixed = T)))
autoiter <- strtoi(substring(filename,lastunders+1,lastdot-1))
iter_weights <- autoiter
node_weights <- read.csv(paste0("IPW_node_", k, "_iter_", iter_weights ,".csv"))$IPW
# Last case is when no weights are provided. Uses uniform weights
} else {
node_weights <- rep(1, n)
}
# Method isn't yet available for missing data
if(any(is.na.data.frame(node_data))){
stop("At least one NA was found in the data. \n The algorithm currently works only with complete data.")
}
# Makes sure the outcome variable is properly coded as 0s and 1s.
if(!all(unique(node_data$out1) %in% c(0,1))){
stop("The outcome variable (out1) contains values that are different from 0 and 1, which isn't allowed.")
}
# Fitting local model to generate an initial local estimator --------------
fit <- glm(out1 ~ ., data=node_data, family="binomial", weights = node_weights)
coefs <- as.vector(fit$coefficients)
coefs
out1
table(node_data$out1)
length(n) <- length(coefs)
write.csv(cbind(coefs, n),
file=paste0("Data_node_",k,"_iter_0_output.csv"),
row.names=FALSE)
# Write variables names
write.csv(colnames(node_data)[-1], file=paste0("Predictor_names_", k, ".csv"), row.names = FALSE)
if (manualwd != 1) {
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically set working directory
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
} else {
print("The automated working directory setup has been bypassed. If there is an error, this might be the cause.")
}
# Verifying if there is a coordination node output file present
nbprimerfiles <- length(list.files(pattern="Coord_node_iter_[[:digit:]]+_primer.csv"))
nbprimerfiles
if (manualwd != 1) {
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically set working directory
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
} else {
print("The automated working directory setup has been bypassed. If there is an error, this might be the cause.")
}
t <- -1
# If there is a manual override, the iteration sequence number (t) is set to the manual value ------------
if (manualt >= 0) {
t <- manualt
# If there is no valid override sequence number, there will be an attempt to extract the number from the data file name
} else {
# List all the data files conforming the the pattern below. There should be at least 1
coordouputfileslist <- list.files(pattern="Coord_node_iter_[[:digit:]]+_primer.csv")
# Assuming there is at least one file found
if (length(coordouputfileslist) > 0) {
itervec=vector(mode="numeric")
for (fl in coordouputfileslist){
outputfname <- fl
underspositions <- unlist(gregexpr("_",outputfname))
lastundersf <- max(underspositions)
beforelastundersf <- underspositions[length(underspositions)-1]
iterfl <- strtoi(substring(outputfname,beforelastundersf+1,lastundersf-1))
itervec <- append(itervec,iterfl)
}
t <- max(itervec)
} else {
stop("There is no primer file found")
}
}
beta_old <- read.csv(paste0("Coord_node_iter_", t, "_primer.csv"))[,1]
K <- length(list.files(pattern=paste0("Data_node_[[:digit:]]+_iter_", t, "_output.csv")))
p <- 0
for (k in 1:K) {
node_k <- read.csv(paste0("Data_node_", k, "_iter_", t, "_output.csv"))
q <- nrow(node_k)
if (p == 0) {
p <- q
D_t <- rep(0, p)
V_t <- matrix(0, p, p)
}
else if (p != q)
stop("Nodes files do not seem to contain the same number of predictors.")
D_t <- D_t + node_k[,1]
V_t <- V_t + as.matrix(node_k[,-1])
}
K <- length(list.files(pattern=paste0("Data_node_[[:digit:]]+_iter_", t, "_output.csv")))
p <- 0
q
node_k
k=1
node_k <- read.csv(paste0("Data_node_", k, "_iter_", t, "_output.csv"))
q <- nrow(node_k)
node_k <- read.csv(paste0("Data_node_", k, "_iter_", t, "_output.csv"))
source("Data_node_call_log-reg_1.R")
source("Data_node_call_log-reg_2.R")
source("Data_node_call_log-reg_3.R")
source("Coord_node_call_iter_log-reg.R")
source("Data_node_call_log-reg_1.R")
k=1
t=1
sigmoid <- function(x) {
exp(x) / (1 + exp(x))
}
logreg_D <- function(beta, X, y, W) {
n <- nrow(X)
t(X) %*% W %*% (y - sigmoid(X %*% beta)) # / n
}
logreg_V <- function(beta, X, W) {
n <- nrow(X)
sig <- sigmoid(X %*% beta)[,1]
t(X) %*% W %*% diag(sig*(1-sig)) %*% X # / n
}
node_data <- read.csv(paste0("Data_node_", k, ".csv"))
n <- nrow(node_data)
# Verifying if weights are available. If not, use values of 1s as uniform weights.
if (file.exists(paste0("Weights_node_", k, ".csv"))) {
node_weights <- read.csv(paste0("Weights_node_", k, ".csv"))[,1]
} else {
node_weights <- rep(1, n)
}
beta_t <- read.csv(paste0("Coord_node_iter_", t, "_primer.csv"))[,1]
X_k <- as.matrix(cbind(1, node_data[,-1]))
y_k <- node_data[,1]
View(X_k)
W_k <- diag(node_weights)
D_k_t <- logreg_D(beta_t, X_k, y_k, W_k)
V_k_t <- logreg_V(beta_t, X_k, W_k)
output <- cbind(D_k_t, V_k_t)
colnames(output)[1] <- "gradient"
colnames(output)[1] <- "gradient"
colnames(output)
output <- as.matric(cbind(D_k_t, V_k_t))
output <- as.matrix(cbind(D_k_t, V_k_t))
colnames(output)[1] <- "gradient"
output <- as.data.frame(cbind(D_k_t, V_k_t))
colnames(output)[1] <- "gradient"
colnames(output)[2] <- "hessian_intercept"
colnames(output)[-c(1,2)] <- paste("hessian", colnames(output)[-c(1,2)], sep = "_")
# QuickRun
maxiter = 10
for(i in 1:maxiter){
source("Data_node_call_log-reg_1.R")
source("Data_node_call_log-reg_2.R")
source("Data_node_call_log-reg_3.R")
source("Coord_node_call_iter_log-reg.R")
}
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/examples/random_data_with_weights_same_folder/pooled")
# Pooling data for comparison with pooled model
data_pooled <- rbind(read.csv(paste0("Data_node_1.csv")),
read.csv(paste0("Data_node_2.csv")),
read.csv(paste0("Data_node_3.csv")))
# Pooling data for comparison with pooled model
weights_pooled <- rbind(read.csv(paste0("Weights_node_1.csv")),
read.csv(paste0("Weights_node_2.csv")),
read.csv(paste0("Weights_node_3.csv")))
write.csv(weights_pooled, file = "Weights_pooled.csv", row.names = F)
# Verifying if weights are available. If not, use values of 1s as uniform weights.
if (file.exists(paste0("Weights_pooled.csv"))) {
weights_pooled <- read.csv("Weights_pooled.csv")[,1]
} else {
weights_pooled <- rep(1, nrow(data_pooled))
}
fit <- glm(out1 ~ ., data=data_pooled, family="binomial", weights = weights_pooled)
print(summary(fit)$coefficients)
print("Confidence intervals")
print(confint.default(fit))
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/MatchIt_data_same_folder/distributed")
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/MatchIt_data/pooled")
############### Distributed inference ####################
############### Demo script ##############################
## License: https://creativecommons.org/licenses/by-nc-sa/4.0/
## Copyright: GRIIS / Universit√© de Sherbrooke
# This assumes three data node files in the same folder named as below
# The output is visible in RStudio console
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically set working directory
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
# Pooling data for comparison with pooled model
data_pooled <- rbind(read.csv(paste0("Data_node_1.csv")),
read.csv(paste0("Data_node_2.csv")),
read.csv(paste0("Data_node_3.csv")))
weights_pooled <- read.csv("Weights_pooled.csv")[,1]
############### Distributed inference ####################
############### Demo script ##############################
## License: https://creativecommons.org/licenses/by-nc-sa/4.0/
## Copyright: GRIIS / Universit√© de Sherbrooke
# This assumes three data node files in the same folder named as below
# The output is visible in RStudio console
# Set working directory automatically
# this.path package is available
if (require(this.path)) {
setwd(this.dir())
# else if running in R studio and the rstudioapi is available, set the correct working directory
} else if ((Sys.getenv("RSTUDIO") == "1") & (require("rstudioapi"))) {
print("RSTUDIO")
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
# no known means to automatically set working directory
} else {
stop("The required conditions to automatically set the working directory are not met. See R file")
}
# Pooling data for comparison with pooled model
data_pooled <- rbind(read.csv(paste0("Data_node_1.csv")),
read.csv(paste0("Data_node_2.csv")),
read.csv(paste0("Data_node_3.csv")))
# Verifying if weights are available. If not, use values of 1s as uniform weights.
if (file.exists(paste0("Weights_pooled.csv"))) {
weights_pooled <- read.csv("Weights_pooled.csv")[,1]
} else {
weights_pooled <- rep(1, nrow(data_pooled))
}
# Fitting and printing pooled model
print("Pooled logistic regression results:")
fit <- glm(Tx ~ ., data=data_pooled, family="binomial", weights = weights_pooled)
print(summary(fit)$coefficients)
print("Confidence intervals")
print(confint.default(fit))
# Predicted probabilities
predict(fit, data_pooled, type="response")
# Predicted probabilities
Weights_save =predict(fit, data_pooled, type="response")
write.csv(as.data.frame(Weights_save), file = "Weights_test.csv", row.names = F)
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/examples/MatchIt_data_with_weights_same_folder/pooled")
setwd("~/GitHub/Elo_Propre/Distrib_analysis/logistic_regression/Data_preparation/Weights_generation_for_IPW/examples/MatchIt_data/pooled")
W1 = Weights_save[1:200]
W2 = Weights_save[201:400]
W3 = Weights_save[-(1:400)]
write.csv(as.data.frame(W1), file = "Weights_node_1", row.names = F)
W1 = Weights_save[1:200]
W2 = Weights_save[201:400]
W3 = Weights_save[-(1:400)]
write.csv(as.data.frame(W1), file = "Weights_node_1", row.names = F)
write.csv(as.data.frame(W2), file = "Weights_node_2", row.names = F)
write.csv(as.data.frame(W3), file = "Weights_node_3", row.names = F)
write.csv(as.data.frame(W1), file = "Weights_node_1.csv", row.names = F)
write.csv(as.data.frame(W2), file = "Weights_node_2.csv", row.names = F)
write.csv(as.data.frame(W3), file = "Weights_node_3.csv", row.names = F)
